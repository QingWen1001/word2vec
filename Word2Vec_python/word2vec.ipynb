{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "### 构建损失器及网络\n",
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入包\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    #读取文件\n",
    "    with open(file_path) as f:\n",
    "        text = f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预处理\n",
    "def preprocess(text,freq):\n",
    "    #英文预料\n",
    "    text = text.lower()\n",
    "    text = text.replace('.','PERIO')\n",
    "    words = text.split()\n",
    "    \n",
    "    #去除低频词\n",
    "    word_counts = Counter(words)\n",
    "    #Counter = [key,value]\n",
    "    trimmed_words = [word for word in words if word_counts[word]>freq]\n",
    "    return trimmed_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备工作：词典、文本转换为数字 、训练样本准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2id(text):\n",
    "    words = preprocess(text)\n",
    "    vocab = set(word)\n",
    "    vocab2id = {w:c for c,w in enumerate(list(vocab))}\n",
    "    id2vacab = {c:w for c,w in enumerate(list(vocab))}\n",
    "    #将语料转换为数字形式\n",
    "    int_words = [vocab2id[w] for w in words]\n",
    "    #对高频词进行处理p(wi) = 1-(t/f(wi))^1/2  每个词去除的概率 t是超参数 f（wi）是 wi 的频率\n",
    "    t = 1e-5 \n",
    "    int_words_counts = Counter(int_words)\n",
    "    total_count = len(int_words)\n",
    "    word_freqs = {w:c/total_count for w,c in int_word_counts.items()}\n",
    "    prob_drop = {w:1-up.sqrt(t/word_freqs[w]) for w in int_word_counts} #计算保留率\n",
    "    train_words = [w for w in int_words if random.random()<(1-prob_drop[w])] #处理完成后用于训练的语料数据\n",
    "    return word_freqs, vicab2id, id2vocab, train_words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取周边词/target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(words, idx , window_size = 5):#窗口大小是不固定的，随机的从窗口阈值里选取1个\n",
    "    target_window = np.random.randint(1,window_size) #从 1-window_size 中选取一个窗口大小\n",
    "    start_point = max(0,idx - target_window)\n",
    "    end_point = idx + target_window\n",
    "    targets = set(words[start_point:idx]+words[idx+1:end_point+1])\n",
    "    return list(targets)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch 迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(words, batch_size, window_size):\n",
    "    n_batches = len(words)//batch_size\n",
    "    #修剪预料至刚好能被batch整除\n",
    "    words = words[:n_batches*batch_size]\n",
    "    #构建 x,y\n",
    "    for idx in range(0,len(words),batch_size):\n",
    "        batch_x, batch_y = [],[]\n",
    "        batch = words[idx:idx+batch_size]  #获取batch\n",
    "        for i in range(len(batch)):\n",
    "            x = batch[i]\n",
    "            y = get_target(batch , i , window_size)\n",
    "            batch_x.extend([x]*len(y)) #虽然一个中心词对应多个周边词，但每次训练时，是一对一对的进行训练的，所以x的数量要与y的数量一样\n",
    "            batch_y.extend(y)\n",
    "        yield batch_x, batch_y  \n",
    "        #yield 将函数变成一个可迭代的 generate 生成器 每次执行函数 会 返回当前的 yield 下次调用会从当前循环继续产生新的返回值 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramNeg(nn.Module):#从 nn.module 继承类\n",
    "    def __init__(self, n_vocab, n_embed, noise_dist = None): #vocab的大小 embed 的大小 noise_distributuon 负采样的参数\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.noise_dist = noise_dist\n",
    "        \n",
    "        #define embedding layers for input and output words\n",
    "        self.in_embed = nn.Embedding(n_vocab, n_embed)\n",
    "        self.out_embed = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        #initialize embedding tables with uniform distribution s使用均匀分布初始化 embedding tables\n",
    "        # i believe this helps with convergence\n",
    "        self.in_embed.weight.data.uniform_(-1,1)\n",
    "        self.out_embed.weight.data.uniform_(-1,1)\n",
    "        \n",
    "    def forward_input(self, input_words):\n",
    "        # input_words  --- onehot\n",
    "        # return  --- embedding vector\n",
    "        input_vectors = self.in_embed(input_words)\n",
    "        return input_vectors\n",
    "    \n",
    "    def forward_output(self, output_words):\n",
    "        output_vectors = self.out_embed(output_words)\n",
    "        return output_vectors\n",
    "    \n",
    "    def forward_noise(self, batch_size, n_samples):\n",
    "        # n_sample 负采样的个数\n",
    "        # Generate noise vectors with shape (batch_size, n_samples, n_embed)\n",
    "        # 负采样的分布，如果没有预先设置的负采样分布 ，就在字典中进行均匀采样，否则使用预先设置的负采样分布\n",
    "        if self.noise_dist is None:\n",
    "            #sample words uniformly\n",
    "            noise_dist = torch.ones_like(self.n_vocab)\n",
    "        else:\n",
    "            noise_dist = self.noise_dist\n",
    "            \n",
    "        # sample words from our noise distribution\n",
    "        noise_words = torch.multinomial(noise_dist, batch_size * n_samples, replacement = True)\n",
    "        #多项式采样\n",
    "        #noise_dist => [0.1, 0.2, 0.2, 0.5] 每个单词对应的分布\n",
    "        #batch_size * n_samples 每个中心点对应的负采样个数\n",
    "        \n",
    "        noise_vectors = self.out_embed(noise_words).view(batch_size, n_samples, self.n_embed)\n",
    "        # .view 将数据整理成 batch_size * n_samples * n_embed 的形式，即每个 中心词 对应的 负采样词 的 词向量    \n",
    "        # 矩阵的形式 矩阵在后续的点积计算中非常重要\n",
    "        return noise_vectors\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用于负采样的单词分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_dist(word_freqs):\n",
    "    word_freqs = np.array(word_freqs.values())\n",
    "    unigram_dist = word_freqs / word_freqs.sum()\n",
    "    noise_dist = torch.form_numpy(unigram_dist ** (0.75)) / np.sum(unigram_dist **(0.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造损失函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegtiveSamplingLoss(nn.Module):\n",
    "    def __init__ (self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input_vectors, output_vectors, noise_vectors):\n",
    "        batch_size , embed_size = input_vectors.shape\n",
    "        \n",
    "        # input vectors should be a batch of column vectors\n",
    "        input_vectors = input_vectors.view(batch_size , embed_size,1)\n",
    "        \n",
    "        # output vectors should be a batch of row vectors\n",
    "        output_vectors = output_vectors.vive(batch_size, 1, embed_size)\n",
    "        \n",
    "        #bmm = batch matrix multiplication 整个 batch 的 embed_vector 进行运算 \n",
    "        # correct log-sigmoid loss\n",
    "        # 此处的 要 output 在前面 input在后面 保证乘出来是 batch_size 个 数\n",
    "        out_loss = torch.bmm(output_vectors,input_vectors).sigmoid().log()\n",
    "        out_loss = out_loss.squeeze()\n",
    "        \n",
    "        #incorrect log-sigmoid loss\n",
    "        # .neg() 是取负号\n",
    "        noise_loss = torch.bmm(noise_vectors.neg(),input_vectors).sigmoid().log()\n",
    "        noise_loss = noise_loss.squeeze().sum(1) # sum the losses over the sample of noise vectors\n",
    "        \n",
    "        # negate and sum correct and noisy log-sigmoid losses\n",
    "        # return average batch loss\n",
    "        return -(out_loss + noise_loss).mean()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型的训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the model\n",
    "embedding_dim = 300\n",
    "noise_dist = get_noise_dist(word_freqs)\n",
    "model = SkipGramNeg(len(vecab2id), embedding_dim, noise_dist= noise_dist)\n",
    "\n",
    "#using the loss that we defined\n",
    "criterion = NegativeSamplingLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "print_every = 1500\n",
    "steps = 0\n",
    "epochs = 5\n",
    "batch_size = 500\n",
    "n_samples = 5\n",
    "\n",
    "#train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    \n",
    "    #get our input,target batches\n",
    "    for input_words, target_words in get_batch(train_words,batch_size):\n",
    "        steps +=1\n",
    "        inputs , targets = torch.logTensor(input_words),torch.LongTensor(target_words)\n",
    "        \n",
    "        #input output and noise vectors\n",
    "        input_vectors = model.forward_input(inputs)\n",
    "        output_vectors = model.forward_output(targets)\n",
    "        noise_vectors = model.forward_noise(batch_size, n_samples)\n",
    "        \n",
    "        #negative sampling loss\n",
    "        loss = criterion(input_vectors, output_vectors, noise_vectors)\n",
    "        if steps//print_every == 0:\n",
    "            print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
